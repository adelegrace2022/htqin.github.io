<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <script type="text/javascript" src="./res/shownews.js"></script>
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a { 
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:191px; height:191px; border-radius:191px; -webkit-border-radius:191px; -moz-border-radius:191px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }

    #hidden_news {
        display: none;
    }

    #hide_news_botton {
        cursor: pointer;
    }

    #hidden_events {
        display: none;
    }

    #hide_events_botton {
        cursor: pointer;
    }
    </style>

    <title>Haotong Qin</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="https://htqin.github.io/Imgs/buaa_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="75%" valign="middle">
                <p align="center"><name>Haotong Qin</name></p>
                <p align="justify">I am a PhD candidate (2019.09-) at the State Key Laboratory of Software Development Environment (SKLSDE)
                    and Shen Yuan Honors College, <a href="https://www.buaa.edu.cn/">Beihang University</a>,
                    supervised by Prof. <a href="https://en.wikipedia.org/wiki/Li_Wei_(computer_scientist)">Wei Li</a>
                    and Prof. <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>. 
                    I will be a visiting PhD student at the <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>, supervised by Prof. <a href="https://www.yf.io/">Fisher Yu</a>.
                    I obtained my BSc degree in Computer Science and Engineering (<i>Summa Cum Laude</i>) from <a href="https://www.buaa.edu.cn/">Beihang University</a> (2015.09-2019.07).
                    <br><br>
                    Now I am an research intern (2021.10-) of <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a>, and I was interned at <a href="https://www.tencent.com/en-us">Weixin Group of Tencent</a> in 2020. 
                    In my undergraduate study, I interned at the <a href="https://www.msra.cn/">Microsoft Research Asia</a>.
                    <br><br>
                    <strong>Email:</strong> qinhaotong@buaa.edu.cn / qinhaotong@gmail.com
                <br>
                
                </p><p align="center">
                    <a href="https://scholar.google.com/citations?user=mK6n-KgAAAAJ">Google Scholar</a> / 
                    <a href="https://github.com/htqin"> Github </a> / 
                    <a href="https://www.linkedin.com/in/haotongqin/">Linkedin</a>
                </p>

              </td>
			  <td align="right"> <img class="hp-photo" src="./Imgs/photo.jpg" style="width: 191;">
                <p style="text-align:center">
                    <a href="https://twitter.com/qin_haotong?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @qin_haotong</a><script async src="https://htqin.github.io/res/widgets.js" charset="utf-8"></script>
                  </p>
              </td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">I'm interested in <strong><i>network binarization and quantization</i></strong>. 
               And my research goal is to enable state-of-the-art neural network models to be deployed on resource-limited hardware, 
               including the compression for different neural architectures, 
               and the flexible deployment on various hardware.  
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Recent News</heading> 
            <p> <strong>[2022.07.12]</strong> I obtain the China Scholarship Council (CSC) scholarship. 
            <p> <strong>[2022.06.29]</strong> One co-authored <a href=" ">paper</a> for ViT quantization is accepted by <a href="https://2022.acmmm.org/">ACM MM 2022</a>. 
            <p> <strong>[2022.06.10]</strong> I obtain the <font color="red">Beihang Top 10 PhD Students Award</font>!
            <p> <strong>[2022.05.18]</strong> Our <a href="https://openreview.net/forum?id=5xEgrl_5FAJ">BiBERT</a> (ICLR'21) integrated into Baidu's open deep learning platform <a href="https://github.com/PaddlePaddle/PaddleSlim/tree/develop/demo/quant/BiBERT">PaddlePaddle</a>.</p>
            <p> <strong>[2022.04.21]</strong> One first-authored <a href="https://arxiv.org/abs/2202.06483">paper</a> for FSMN binarization is accepted by <a href="https://cvpr2022.thecvf.com/">IJCAI 2022</a>. 
            <p> <strong>[2022.03.16]</strong> Our survey <a href="https://www.sciencedirect.com/science/article/pii/S0031320320300856">paper</a> for binary neural networks is selected to <font color="red">ESI Highly Cited Papers</font></a>.
            <p> <strong>[2022.03.02]</strong> One co-authored <a href=" ">paper</a> for physical world robustness is accepted by <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>. 
            <div id="hide_news_botton">
                <p><a onclick="showNews()">[view more]</a> </p>
            </div>
            <div id="hidden_news">
                <p> <strong>[2022.01.21]</strong> One first-authored <a href="https://openreview.net/forum?id=5xEgrl_5FAJ">paper</a> for BERT binarization is accepted by <a href="https://iclr.cc/">ICLR 2022</a>. 
                <p> <strong>[2021.10.13]</strong> I join Bytedance AI Lab as an research intern.</p>
                <p> <strong>[2021.09.27]</strong> Our <a href="https://arxiv.org/abs/2010.05501">BiPointNet</a> (ICLR'21) obtain the Most Popular Paper in Beijing Area.</p>
                <p> <strong>[2021.09.20]</strong> I obtain China National Scholarship (the 2nd time).</p>
                <p> <strong>[2021.07.23]</strong> One co-authored <a href=" ">paper</a> for object detection is accepted by <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
                <p> <strong>[2021.05.17]</strong> I obtain Beihang-Huawei Scholarship.</p>
                <p> <strong>[2021.03.01]</strong> One co-authored <font color="red"><strong>oral</strong></font> <a href="https://arxiv.org/pdf/2013.01049.pdf">paper</a> for data-free quantization is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.    
                <p> <strong>[2021.01.13]</strong> One first-authored <a href="https://openreview.net/pdf?id=9QLRCVysdlO">paper</a> for PointNet binarization is accepted by <a href="https://iclr.cc/">ICLR 2021</a>. </p>
                <p> <strong>[2020.09.20]</strong> I obtain China National Scholarship.</p>
                <p> <strong>[2020.09.18]</strong> I release our open source project <a href="https://github.com/htqin/awesome-model-quantization">"Awesome Model Quantization"</a>.</p>
                <p> <strong>[2020.06.23]</strong> I join Tencent WXG as an research intern.</p>
                <p> <strong>[2020.04.14]</strong> I am invited to present our <a href="https://arxiv.org/abs/1909.10788">IR-Net</a> and <a href="https://arxiv.org/abs/2004.03333">survey paper</a> at JD.com, Inc. Here are the <a href="https://htqin.github.io/Slides/talk-JD-20200414.pptx">Slides</a>. </p>
                <p> <strong>[2020.02.28]</strong> One first-authored <a href="https://arxiv.org/abs/1909.10788">paper</a> for model binarization is accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. </p>
                <p> <strong>[2020.02.27]</strong> One co-authored paper for video hashing is accepted by <a href="https://ieee-cas.org/publications/transactions-multimedia">TMM</a>. </p>
                <p> <strong>[2020.02.20]</strong> One first-authored <a href="https://www.sciencedirect.com/science/article/pii/S0031320320300856">survey paper</a> for binary neural networks is accepted by <a href="https://www.journals.elsevier.com/pattern-recognition">PR</a>. </p>
                <p> <strong>[2018.11.07]</strong> I join MSRA as an research intern.</p>
            </div>
         </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Recent Events</heading> 
            <p> <strong>[Student-forum@VALSE2022]</strong> I am co-organizing the <a href="http://valser.org/2022/#/workshop">student forum</a> at VALSE 2022.
            <div id="hide_events_botton">
                <p><a onclick="showEvents()">[view more]</a> </p>
            </div>
            <div id="hidden_events">
                <p> <strong>[Workshop@CVPR2022]</strong> I am co-organizing the workshop on <a href="https://artofrobust.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> at CVPR 2022.
                <p> <strong>[Workshop@AAAI2022]</strong> I am co-organizing the 1st international workshop on <a href="https://practical-dl.github.io/">The Practical Deep Learning in the Wild (PracticalDL-22)</a> at AAAI 2022.
                <p> <strong>[Thematic-forum@PRCV2022]</strong> I am co-organizing the thematic forum on The Hardware-friendly Lightweight Deep Learning at PRCV 2021.
            </div>
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <td>
          <heading>Selected Publications<a name="publications"></a>
        </heading>
        <p>
        You can find the full list on <a href="https://scholar.google.com/citations?user=mK6n-KgAAAAJ">Google Scholar</a> and <a href="https://xlliu-beihang.github.io/publication.html">our group publication page</a>.
        </p>
        </td></tbody>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
		<tbody>

        <tr><td width="20%"><img src="./Imgs/bibert.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://openreview.net/forum?id=5xEgrl_5FAJ">
                <papertitle>BiBERT: Accurate Fully Binarized BERT</papertitle></a>
                [<a href="https://openreview.net/pdf?id=5xEgrl_5FAJ">PDF</a>]
                <br><strong>Haotong Qin</strong>, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2203.06390">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/gQJb5YhrNYgA-JK3AGeUdQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/uhIf3MEWmSjnG5M_a5a1iA"><font color="red">百度) </font></a> /
                <a href="https://github.com/htqin/BiBERT"><font color="red">Torch</font></a>,
                <a href="https://github.com/PaddlePaddle/PaddleSlim/tree/develop/demo/quant/BiBERT"><font color="red">PaddlePaddle</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=PaddlePaddle&repo=PaddleSlim&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">In this paper, we propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks.
                    BiBERT introduces an efficient Bi-Attention structure and a DMD scheme, which yields impressive 59.2x and 31.2x saving on FLOPs and model size.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/bifsmn.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2202.06483">
                <papertitle>BiFSMN: Binary Neural Network for Keyword Spotting</papertitle></a>
                [<a href="https://arxiv.org/pdf/2202.06483.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Xudong Ma, Yifu Ding, Xiaoyang Li, Yang Zhang, Yao Tian, Zejun Ma, Jie Luo, Xianglong Liu
                <br>
                <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2202.06483">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/TXItTmkVAk_BMyQ7eb7WAg"><font color="red">(机器之心, </font></a>
                <a href="https://mp.weixin.qq.com/s/epGPf15BYRmamBvXrbs0NA"><font color="red">PaperWeekly) </font></a>
                <p align="justify" style="font-size:13px">In this paper, we present BiFSMN, an accurate and extreme-efficient binary network for KWS, 
                    outperforming existing methods on various KWS datasets and achieving impressive 22.3x speedup and 15.5x storage-saving on edge hardware.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/APQ-ViT.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href=" ">
                <papertitle>Towards Accurate Post-Training Quantizationfor Vision Transformer</papertitle></a>
                [<a href=" ">PDF</a>]
                <br>Yifu Ding, <strong>Haotong Qin</strong>, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, Xianglong Liu
                <br>
                <em>ACM Multimedia (ACM MM)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2202.06483">arXiv</a> 
                <p align="justify" style="font-size:13px">We propose a novel Accurate Post-training Quantization framework for Vision Transformer, namely APQ-ViT, 
                    which surpasses the existing post-training quantization methods by convincing margins, especially in lower bit settings. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/bipointnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2010.05501">
                <papertitle>BiPointNet: Binary Neural Network for Point Clouds</papertitle></a>
                [<a href="https://arxiv.org/pdf/2010.05501.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2010.05501">arXiv</a> / 
                <a href="https://htqin.github.io/Projects/BiPointNet.html"><font color="red">Project</font></a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/W13lZOWAzw03PqafTteP_Q"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/u84BaMXXJyujKM-Oaj0HBQ"><font color="red">商汤学术) </font></a> /
                <a href="https://github.com/htqin/BiPointNet"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>
                <em><font color="red">2021 The Most Popular Papers in Beijing Area</font></em> (Field: Graphic and Image)
                </p><p></p>
                <p align="justify" style="font-size:13px">We presented BiPointNet, the first model binarization approach for efficient deep learning on point clouds. 
                    BiPointNet gave an impressive 14.7x speedup and 18.9x storage saving on real-world resource-constrained devices.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/dsg.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2013.01049">
                <papertitle>Diversifying Sample Generation for Accurate Data-Free Quantization</papertitle></a>
                [<a href="https://arxiv.org/pdf/2013.01049.pdf">PDF</a>]
                <br>Xiangguo Zhang, <strong>Haotong Qin</strong>, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu
                <br>
                <em style="font-size:13px">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> <font color="red"><strong style="font-size:13px">Oral</strong></font>, <font style="font-size:13px">2021</font>
                <br>
                <a href="https://arxiv.org/abs/2013.01049">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/0qIBM4wJTc12WzghV1V_gQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/WftpvEWa_BAyljyyRSIEVQ"><font color="red">商汤学术) </font></a>
                <br>
                <p align="justify" style="font-size:13px">We proposed Diverse Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization in data-free quantization, 
                    which obtained significant improvements over various networks and quantization methods.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/ijcaidc21.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2112.00737">
                <papertitle>Hardware-friendly Deep Learning by Network Quantization and Binarization</papertitle></a>
                [<a href="https://arxiv.org/pdf/2112.00737.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>
                <br>
                <em>International Joint Conference on Artificial Intelligence (IJCAI) Doctoral Consortium</em>, <font style="font-size:13px">2021</font>
                <br>
                <a href="https://arxiv.org/abs/2112.00737">arXiv</a>
                <br>
                <p align="justify" style="font-size:13px">We summarize challenges of quantization into two categories: Quantization for Diverse Architectures and Quantization on Complex Scenes. 
                    Our studies focus mainly on applying quantization on various architectures and scenes and pushing the limit of quantization to extremely compress and accelerate networks.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/cvpr2020_6014.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1909.10788">
                <papertitle>Forward and Backward Information Retention for Accurate Binary Neural Networks</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/QIN_CVPR2020_6014.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, Jingkuan Song
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1909.10788">arXiv</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/cF14wwgnMcnvkBa864ox1Q"><font color="red">(机器之心, </font></a>
                <a href="https://mp.weixin.qq.com/s/Sy42uvEpb6HANZqlXx1q0w"><font color="red">商汤学术, </font></a>
                <a href="https://mp.weixin.qq.com/s/I-MaaAmrcKd7-ATHmMtuwQ"><font color="red">CVer, </font></a>
                <a href="https://mp.weixin.qq.com/s/tglJjwCfAfa8UTydBMbqgA"><font color="red">AI科技大本营)</font></a> / 
                <a href="https://github.com/htqin/IR-Net"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=IR-Net&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We proposed a novel Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients,
                    and we were the first to implement and report 1-bit BNN speed on edge devices. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/pr2020_survey.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320300856">
                <papertitle>Binary Neural Networks: A Survey</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/pr2020_BNN_survey.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu Sebe<br>
                <em>Pattern Recognition (PR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/2004.03333">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/QGva6fow9tad_daZ_G2p0Q"><font color="red">(PaperWeekly, </font></a>
                <a href="https://www.jiqizhixin.com/dailies/9c9fde93-8c87-4067-a4bd-f4815fafc49b"><font color="red">机器之心)</font></a> / 
                <a href="https://github.com/htqin/awesome-model-quantization"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=awesome-model-quantization&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>
                <em><font color="red">ESI Highly Cited Papers</font></em> (2022)
                </p><p></p>
                <p align="justify" style="font-size:13px">We presented a comprehensive survey of BNNs. 
                    We also investigated other practical aspects of binary neural networks such as the hardware-friendly design and training tricks. 
                    Then, we gave the evaluation and discussions on different tasks. 
                    Finally, the challenges may be faced in future research were prospected.</p>
            </td>
        </tr>

        </tbody>
    </table>
    

    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Main Honors and Awards</heading>

             <p> <strong>[2022.07]</strong> &nbsp;&nbsp; China CSC scholarship (<strong>5500 people nationwide</strong>)</p>

             <p> <strong>[2022.06]</strong> &nbsp;&nbsp; <font color="red">Beihang Top 10 PhD Students Award</font> (<strong>10 people in Beihang University</strong>)</p>

             <p> <strong>[2022.05]</strong> &nbsp;&nbsp; Beihang Youth May 4th Medal Nomination (<strong>Top Honor</strong> for Beihang Young People)</p>

             <p> <strong>[2021.09]</strong> &nbsp;&nbsp; The Most Popular Paper in Beijing Area (Field: Graphic and Image)</p>

             <p> <strong>[2021.09]</strong> &nbsp;&nbsp; China National Scholarship (<strong>the 2nd time</strong>)</p>
            
             <p> <strong>[2021.05]</strong> &nbsp;&nbsp; Huawei Scholarship (Top 1%)</p>

             <p> <strong>[2020.09]</strong> &nbsp;&nbsp; China National Scholarship (Top 1%)</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Tencent Rhino-Bird Elite (<strong>51 people worldwide</strong>)</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Shen Yuan Honors College at Beihang University (Top 1%)</p>

             <p> <strong>[2019.04]</strong> &nbsp;&nbsp; ICPC China National Invitational Contest (<strong>Gold Medal</strong>)</p>

             <p> <strong>[2018.03]</strong> &nbsp;&nbsp; ACM-ICPC Chinese Collegiate Programming Contest (<strong>Gold Medal</strong>)</p>

             <p> <strong>[2016.07]</strong> &nbsp;&nbsp; International Concert of Chinese Folk Music (<strong>Gold Medal</strong>)</p>

            </td>
            </tr></tbody>
    </table>

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Academic Services</heading>
            <p> <strong>Organizer</strong> of Workshop: PRCV 2021, AAAI 2021, CVPR 2022, VALSE 2022.</p>
            <p> <strong>Reviewer</strong> of Journals: T-PAMI, T-IP, T-NNLS, T-MM, Pattern Recognition, JVCI, JCST, etc.</p>
            <p> <strong>Program Committee</strong> of Conferences: ACM MM 2021/2022, IJCAI 2022, etc.</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Talks and Teaching</heading>
            <p style="font-size:13px"> <strong>[2022.06]</strong> I am invited to host the VALSE Student Webinar & Panel namely "When CV meets NLP".
                 [<a href="https://www.bilibili.com/video/BV1zF411V7cE?share_source=copy_web">Media</a>] </p>
            <p style="font-size:13px"> <strong>[2022.01]</strong> I am invited to host the VALSE Student Webinar about conference and journal rebuttal.
                 [<a href="https://mp.weixin.qq.com/s/PvtzVhbpcP03mkcfFxGLGw">Media</a>] </p>
            <p style="font-size:13px"> <strong>[2021.07]</strong> I am invited to talk about Network Quantization in Multiple Scenarios at 
                J Ventures (将门创投). [<a href="https://www.techbeat.net/talk-info?id=565">Video</a>] </p>
            <p style="font-size:13px"> <strong>[2021.06]</strong> I am invited to talk about Data-free Quantization at 
                Zhidx (智东西公开课). [<a href="https://course.zhidx.com/c/NmUxMjIxNzhkZGZhOGUyMzY4YzM=">Video</a>] </p>
            <p style="font-size:13px"> <strong>[2021.05]</strong> I am invited to present our <a href="https://arxiv.org/abs/2013.01049">DSG</a> (CVPR 2021 oral) at 
                MSRA Tech Talk. [<a href="https://htqin.github.io/Slides/DSG-MSRA-20201512.pdf">Slides</a>] </p>
            <p style="font-size:13px"> <strong>[2020.04]</strong> I am invited to present our <a href="https://arxiv.org/abs/1909.10788">IR-Net</a> (CVPR 2020) and <a href="https://arxiv.org/abs/2004.03333">survey paper</a> at 
                JD AI Research. [<a href="https://htqin.github.io/Slides/talk-JD-20200414.pdf">Slides</a>] </p>
            <p style="font-size:13px"> <strong>[Fall 2020]</strong> I am the teaching Assistant in Machine Learning (Beihang University).</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like playing Chinese folk music, especially string music (Erhu, Zhonghu, etc.).
               In fact, I am almost a professional Erhu performer.
               I have studied Erhu supervised by Prof. <a href="https://www.ccom.edu.cn/jxyx/myx/myxls/201510/t20151026_35707.html">Zaili Tian</a>, Prof. <a href="https://baike.baidu.com/item/%E9%AB%98%E6%89%AC/5994407">Yang Gao</a>, and  Prof. Qingfu Zhu. 
               I was the vice-president of the Beihang Folk Music Orchestra, here are some of the performance videos of our orchestra [<a href="https://v.qq.com/x/search/?q=%E5%8C%97%E8%88%AA%E6%B0%91%E4%B9%90%E5%9B%A2&stag=txt.playpage.vppdesc">Tencent Video</a>][<a href="https://www.bilibili.com/video/BV1W44y1v7DB?share_source=copy_web">Bilibili</a>].
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=O7AjyGXmrOPjngfk_VI9NhWFu7JWskLJlRerwKQdjh4"></script>
    </p></td>
    </tr>
    </tbody>
    </table>
