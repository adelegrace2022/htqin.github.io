<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <script type="text/javascript" src="./res/shownews.js"></script>
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a { 
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:191px; height:191px; border-radius:191px; -webkit-border-radius:191px; -moz-border-radius:191px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }

    #hidden_news {
        display: none;
    }

    #hide_news_botton {
        cursor: pointer;
    }

    #hidden_events {
        display: none;
    }

    #hide_events_botton {
        cursor: pointer;
    }
    </style>

    <title>Haotong Qin</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="https://htqin.github.io/Imgs/buaa_icon.jpg">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 1 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="75%" valign="middle">
                <p align="center"><name>Haotong Qin</name></p>
                <p align="justify">I am a PhD candidate (2019.09-) at the State Key Laboratory of Software Development Environment (SKLSDE)
                    and Shen Yuan Honors College, <a href="https://www.buaa.edu.cn/">Beihang University</a>,
                    supervised by Prof. <a href="https://en.wikipedia.org/wiki/Li_Wei_(computer_scientist)">Wei Li</a>
                    and Prof. <a href="https://xlliu-beihang.github.io/">Xianglong Liu</a>. 
                    I will be a visiting PhD student (2022.10-) at the <a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>, supervised by Prof. <a href="https://www.yf.io/">Fisher Yu</a>.
                    I obtained my BSc degree in Computer Science and Engineering (<i>Summa Cum Laude</i>) from <a href="https://www.buaa.edu.cn/">Beihang University</a> (2015.09-2019.07).
                    <br><br>
                    Now I am an research intern (2021.10-) of <a href="https://ailab.bytedance.com/">Bytedance AI Lab</a>, and I was interned at <a href="https://www.tencent.com/en-us">Weixin Group of Tencent</a> in 2020. 
                    In my undergraduate study, I interned at the <a href="https://www.msra.cn/">Microsoft Research Asia</a>.
                    <br><br>
                    <strong>Email:</strong> qinhaotong@buaa.edu.cn / qinhaotong@gmail.com
                <br>
                
                </p><p align="center">
                    <a href="https://scholar.google.com/citations?user=mK6n-KgAAAAJ">Google Scholar</a> / 
                    <a href="https://github.com/htqin"> Github </a> / 
                    <a href="https://www.linkedin.com/in/haotongqin/">Linkedin</a>
                </p>

              </td>
			  <td align="right"> <img class="hp-photo" src="./Imgs/photo.jpg" style="width: 191;">
                <p style="text-align:center">
                    <a href="https://twitter.com/qin_haotong?ref_src=twsrc%5Etfw" class="twitter-follow-button" data-show-count="false">Follow @qin_haotong</a><script async src="https://htqin.github.io/res/widgets.js" charset="utf-8"></script>
                  </p>
              </td></tr>
            </tbody>
          </table>

    <!--SECTION 2 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>Research</heading>
            <p align="justify">I'm interested in <strong><i>network binarization and quantization</i></strong>. 
               And my research goal is to enable state-of-the-art neural network models to be deployed on resource-limited hardware, 
               including the compression for different neural architectures, 
               and the flexible deployment on various hardware.  
		   </td></tr>
       </tbody>
    </table>

    <!--SECTION 3 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Recent News</heading> 
            <p> <strong>[2022.07.12]</strong> I obtain the China Scholarship Council (CSC) scholarship. 
            <p> <strong>[2022.06.29]</strong> One co-authored <a href=" ">paper</a> for ViT quantization is accepted by <a href="https://2022.acmmm.org/">ACM MM 2022</a>. 
            <p> <strong>[2022.06.10]</strong> I obtain the <font color="red">Beihang Top 10 PhD Students Award</font>!
            <p> <strong>[2022.05.18]</strong> Our <a href="https://openreview.net/forum?id=5xEgrl_5FAJ">BiBERT</a> (ICLR'21) integrated into Baidu's open deep learning platform <a href="https://github.com/PaddlePaddle/PaddleSlim/tree/develop/demo/quant/BiBERT">PaddlePaddle</a>.</p>
            <p> <strong>[2022.04.21]</strong> One first-authored <a href="https://arxiv.org/abs/2202.06483">paper</a> for FSMN binarization is accepted by <a href="https://cvpr2022.thecvf.com/">IJCAI 2022</a>. 
            <p> <strong>[2022.03.16]</strong> Our survey <a href="https://www.sciencedirect.com/science/article/pii/S0031320320300856">paper</a> for binary neural networks is selected to ESI Highly Cited Papers.
            <p> <strong>[2022.03.02]</strong> One co-authored <a href=" ">paper</a> for physical world robustness is accepted by <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>. 
            <div id="hide_news_botton">
                <p><a onclick="showNews()">[view more]</a> </p>
            </div>
            <div id="hidden_news">
                <p> <strong>[2022.01.21]</strong> One first-authored <a href="https://openreview.net/forum?id=5xEgrl_5FAJ">paper</a> for BERT binarization is accepted by <a href="https://iclr.cc/">ICLR 2022</a>. 
                <p> <strong>[2021.10.13]</strong> I join Bytedance AI Lab as an research intern.</p>
                <p> <strong>[2021.09.27]</strong> Our <a href="https://arxiv.org/abs/2010.05501">BiPointNet</a> (ICLR'21) obtain the Most Popular Paper in Beijing Area.</p>
                <p> <strong>[2021.09.20]</strong> I obtain China National Scholarship (the 2nd time).</p>
                <p> <strong>[2021.07.23]</strong> One co-authored <a href=" ">paper</a> for object detection is accepted by <a href="http://iccv2021.thecvf.com/">ICCV 2021</a>.
                <p> <strong>[2021.05.17]</strong> I obtain Beihang-Huawei Scholarship.</p>
                <p> <strong>[2021.03.01]</strong> One co-authored <a href="https://arxiv.org/pdf/2013.01049.pdf">oral paper</a> for data-free quantization is accepted by <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.    
                <p> <strong>[2021.01.13]</strong> One first-authored <a href="https://openreview.net/pdf?id=9QLRCVysdlO">paper</a> for PointNet binarization is accepted by <a href="https://iclr.cc/">ICLR 2021</a>. </p>
                <p> <strong>[2020.09.20]</strong> I obtain China National Scholarship.</p>
                <p> <strong>[2020.09.18]</strong> I release our open source project <a href="https://github.com/htqin/awesome-model-quantization">"Awesome Model Quantization"</a>.</p>
                <p> <strong>[2020.06.23]</strong> I join Tencent WXG as an research intern.</p>
                <p> <strong>[2020.04.14]</strong> I am invited to present our <a href="https://arxiv.org/abs/1909.10788">IR-Net</a> and <a href="https://arxiv.org/abs/2004.03333">survey paper</a> at JD.com, Inc. Here are the <a href="https://htqin.github.io/Slides/talk-JD-20200414.pptx">Slides</a>. </p>
                <p> <strong>[2020.02.28]</strong> One first-authored <a href="https://arxiv.org/abs/1909.10788">paper</a> for model binarization is accepted by <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>. </p>
                <p> <strong>[2020.02.27]</strong> One co-authored paper for video hashing is accepted by <a href="https://ieee-cas.org/publications/transactions-multimedia">TMM</a>. </p>
                <p> <strong>[2020.02.20]</strong> One first-authored <a href="https://www.sciencedirect.com/science/article/pii/S0031320320300856">survey paper</a> for binary neural networks is accepted by <a href="https://www.journals.elsevier.com/pattern-recognition">PR</a>. </p>
                <p> <strong>[2018.11.07]</strong> I join MSRA as an research intern.</p>
            </div>
         </td>
       </tr></tbody>
    </table>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Recent Events</heading> 
            <p> <strong>[Workshop@VALSE2022]</strong> I am co-organizing the <a href="http://valser.org/2022/#/workshop">student workshop</a> at VALSE 2022.
            <div id="hide_events_botton">
                <p><a onclick="showEvents()">[view more]</a> </p>
            </div>
            <div id="hidden_events">
                <p> <strong>[Workshop@CVPR2022]</strong> I am co-organizing the workshop on <a href="https://artofrobust.github.io/">The Art of Robustness: Devil and Angel in Adversarial Machine Learning</a> at CVPR 2022.
                <p> <strong>[Workshop@AAAI2022]</strong> I am co-organizing the 1st international workshop on <a href="https://practical-dl.github.io/">The Practical Deep Learning in the Wild (PracticalDL-22)</a> at AAAI 2022.
                <p> <strong>[Thematic-forum@PRCV2022]</strong> I am co-organizing the thematic forum on The Hardware-friendly Lightweight Deep Learning at PRCV 2021.
            </div>
            </td>
       </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <td>
          <heading>Selected Publications<a name="publications"></a>
        </heading>
        <p>
        You can find the full list on <a href="https://scholar.google.com/citations?user=mK6n-KgAAAAJ">Google Scholar</a> and <a href="https://xlliu-beihang.github.io/publication.html">our group publication page</a>.
        </p>
        </td></tbody>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
		<tbody>

        <tr><td width="20%"><img src="./Imgs/bibert.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://openreview.net/forum?id=5xEgrl_5FAJ">
                <papertitle>BiBERT: Accurate Fully Binarized BERT</papertitle></a>
                [<a href="https://openreview.net/pdf?id=5xEgrl_5FAJ">PDF</a>]
                <br><strong>Haotong Qin</strong>, Yifu Ding, Mingyuan Zhang, Qinghua Yan, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2203.06390">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/gQJb5YhrNYgA-JK3AGeUdQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/uhIf3MEWmSjnG5M_a5a1iA"><font color="red">百度) </font></a> /
                <a href="https://github.com/htqin/BiBERT"><font color="red">Torch</font></a>,
                <a href="https://github.com/PaddlePaddle/PaddleSlim/tree/develop/demo/quant/BiBERT"><font color="red">PaddlePaddle</font></a>
                <iframe src="https://ghbtns.com/github-btn.html?user=PaddlePaddle&repo=PaddleSlim&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">In this paper, we propose BiBERT, an accurate fully binarized BERT, to eliminate the performance bottlenecks.
                    BiBERT introduces an efficient Bi-Attention structure and a DMD scheme, which yields impressive 59.2x and 31.2x saving on FLOPs and model size.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/bifsmn.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2202.06483">
                <papertitle>BiFSMN: Binary Neural Network for Keyword Spotting</papertitle></a>
                [<a href="https://arxiv.org/pdf/2202.06483.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Xudong Ma, Yifu Ding, Xiaoyang Li, Yang Zhang, Yao Tian, Zejun Ma, Jie Luo, Xianglong Liu
                <br>
                <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2202.06483">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/TXItTmkVAk_BMyQ7eb7WAg"><font color="red">(机器之心, </font></a>
                <a href="https://mp.weixin.qq.com/s/epGPf15BYRmamBvXrbs0NA"><font color="red">PaperWeekly) </font></a>
                <p align="justify" style="font-size:13px">In this paper, we present BiFSMN, an accurate and extreme-efficient binary network for KWS, 
                    outperforming existing methods on various KWS datasets and achieving impressive 22.3x speedup and 15.5x storage-saving on edge hardware.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/APQ-ViT.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href=" ">
                <papertitle>Towards Accurate Post-Training Quantizationfor Vision Transformer</papertitle></a>
                [<a href=" ">PDF</a>]
                <br>Yifu Ding, <strong>Haotong Qin</strong>, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, Xianglong Liu
                <br>
                <em>ACM Multimedia (ACM MM)</em>, 2022
                <br>
                <a href="https://arxiv.org/abs/2202.06483">arXiv</a> 
                <p align="justify" style="font-size:13px">We propose a novel Accurate Post-training Quantization framework for Vision Transformer, namely APQ-ViT, 
                    which surpasses the existing post-training quantization methods by convincing margins, especially in lower bit settings. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/bipointnet.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2010.05501">
                <papertitle>BiPointNet: Binary Neural Network for Point Clouds</papertitle></a>
                [<a href="https://arxiv.org/pdf/2010.05501.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Zhongang Cai, Mingyuan Zhang, Yifu Ding, Haiyu Zhao, Shuai Yi, Xianglong Liu, Hao Su
                <br>
                <em>International Conference on Learning Representations (ICLR)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2010.05501">arXiv</a> / 
                <a href="https://htqin.github.io/Projects/BiPointNet.html"><font color="red">Project</font></a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/W13lZOWAzw03PqafTteP_Q"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/u84BaMXXJyujKM-Oaj0HBQ"><font color="red">商汤学术) </font></a> /
                <a href="https://github.com/htqin/BiPointNet"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=BiPointNet&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>
                <em><font color="red">2021 The Most Popular Papers in Beijing Area</font></em> (Field: Graphic and Image)
                </p><p></p>
                <p align="justify" style="font-size:13px">We presented BiPointNet, the first model binarization approach for efficient deep learning on point clouds. 
                    BiPointNet gave an impressive 14.7x speedup and 18.9x storage saving on real-world resource-constrained devices.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/dsg.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/2013.01049">
                <papertitle>Diversifying Sample Generation for Accurate Data-Free Quantization</papertitle></a>
                [<a href="https://arxiv.org/pdf/2013.01049.pdf">PDF</a>]
                <br>Xiangguo Zhang*, <strong>Haotong Qin</strong>*, Yifu Ding, Ruihao Gong, Qinghua Yan, Renshuai Tao, Yuhang Li, Fengwei Yu, Xianglong Liu
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                <br>
                <em><font color="red">Oral presentation, Acceptance Rate 4.7%</font></em>
                <br>
                <a href="https://arxiv.org/abs/2013.01049">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/0qIBM4wJTc12WzghV1V_gQ"><font color="red">(量子位, </font></a>
                <a href="https://mp.weixin.qq.com/s/WftpvEWa_BAyljyyRSIEVQ"><font color="red">商汤学术) </font></a>
                <br>
                <p align="justify" style="font-size:13px">We proposed Diverse Sample Generation (DSG) scheme to mitigate the adverse effects caused by homogenization in data-free quantization, 
                    which obtained significant improvements over various networks and quantization methods.</p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/cvpr2020_6014.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	            <p><a href="https://arxiv.org/abs/1909.10788">
                <papertitle>Forward and Backward Information Retention for Accurate Binary Neural Networks</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/QIN_CVPR2020_6014.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, Jingkuan Song
                <br>
                <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/1909.10788">arXiv</a> /
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/cF14wwgnMcnvkBa864ox1Q"><font color="red">(机器之心, </font></a>
                <a href="https://mp.weixin.qq.com/s/Sy42uvEpb6HANZqlXx1q0w"><font color="red">商汤学术, </font></a>
                <a href="https://mp.weixin.qq.com/s/I-MaaAmrcKd7-ATHmMtuwQ"><font color="red">CVer, </font></a>
                <a href="https://mp.weixin.qq.com/s/tglJjwCfAfa8UTydBMbqgA"><font color="red">AI科技大本营)</font></a> / 
                <a href="https://github.com/htqin/IR-Net"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=IR-Net&type=star&count=true&size=small"
                    frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                </p><p></p>
                <p align="justify" style="font-size:13px">We proposed a novel Information Retention Network (IR-Net) to retain the information that consists in the forward activations and backward gradients,
                    and we were the first to implement and report 1-bit BNN speed on edge devices. </p>
                <p></p>
            </td>
        </tr>

        <tr><td width="20%"><img src="./Imgs/pr2020_survey.png" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                <p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320320300856">
                <papertitle>Binary Neural Networks: A Survey</papertitle></a>
                [<a href="https://htqin.github.io/Pubs/pr2020_BNN_survey.pdf">PDF</a>]
                <br><strong>Haotong Qin</strong>, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu Sebe<br>
                <em>Pattern Recognition (PR)</em>, 2020
                <br>
                <a href="https://arxiv.org/abs/2004.03333">arXiv</a> / 
                <font color="red"> News:</font>
                <a href="https://mp.weixin.qq.com/s/QGva6fow9tad_daZ_G2p0Q"><font color="red">(PaperWeekly, </font></a>
                <a href="https://www.jiqizhixin.com/dailies/9c9fde93-8c87-4067-a4bd-f4815fafc49b"><font color="red">机器之心)</font></a> / 
                <a href="https://github.com/htqin/awesome-model-quantization"><font color="red">Torch</font></a> 
                <iframe src="https://ghbtns.com/github-btn.html?user=htqin&repo=awesome-model-quantization&type=star&count=true&size=small"
                frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
                <br>
                <em><font color="red">ESI Highly Cited Papers</font></em> (2022)
                </p><p></p>
                <p align="justify" style="font-size:13px">We presented a comprehensive survey of BNNs. 
                    We also investigated other practical aspects of binary neural networks such as the hardware-friendly design and training tricks. 
                    Then, we gave the evaluation and discussions on different tasks. 
                    Finally, the challenges may be faced in future research were prospected.</p>
            </td>
        </tr>

        </tbody>
    </table>
    

    <!--SECTION 6 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><heading>Main Honors and Awards</heading>

             <p> <strong>[2022.07]</strong> &nbsp;&nbsp; China CSC Scholarship (<strong>5500 people nationwide</strong>)</p>

             <p> <strong>[2022.06]</strong> &nbsp;&nbsp; <font color="red">Beihang Top 10 PhD Students Award</font> (<strong>10 people in Beihang University</strong>)</p>

             <p> <strong>[2022.05]</strong> &nbsp;&nbsp; Beihang Youth May 4th Medal Nomination (<strong>Top Honor</strong> for Beihang Young People)</p>

             <p> <strong>[2021.09]</strong> &nbsp;&nbsp; The Most Popular Paper in Beijing Area (Field: Graphic and Image)</p>

             <p> <strong>[2021.09]</strong> &nbsp;&nbsp; China National Scholarship (<strong>the 2nd time</strong>)</p>
            
             <p> <strong>[2021.05]</strong> &nbsp;&nbsp; Huawei Scholarship (Top 1%)</p>

             <p> <strong>[2020.09]</strong> &nbsp;&nbsp; China National Scholarship (Top 1%)</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Tencent Rhino-Bird Elite (<strong>51 people worldwide</strong>)</p>

             <p> <strong>[2019.10]</strong> &nbsp;&nbsp; Shen Yuan Honors College at Beihang University (Top 1%)</p>

             <p> <strong>[2019.04]</strong> &nbsp;&nbsp; ICPC China National Invitational Contest (<strong>Gold Medal</strong>)</p>

             <p> <strong>[2018.03]</strong> &nbsp;&nbsp; ACM-ICPC Chinese Collegiate Programming Contest (<strong>Gold Medal</strong>)</p>

             <p> <strong>[2016.07]</strong> &nbsp;&nbsp; International Concert of Chinese Folk Music (<strong>Gold Medal</strong>)</p>

            </td>
            </tr></tbody>
    </table>

    <!--SECTION 8 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Academic Services</heading>
            <p> <strong>Organizer</strong> of Workshop: PRCV 2021, AAAI 2021, CVPR 2022, VALSE 2022.</p>
            <p> <strong>Reviewer</strong> of Journals: T-PAMI, T-IP, T-NNLS, T-MM, Pattern Recognition, JVCI, JCST, etc.</p>
            <p> <strong>Program Committee</strong> of Conferences: ACM MM 2021/2022, IJCAI 2022, etc.</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
           <td><heading>Talks and Teaching</heading>
            <p style="font-size:13px"> <strong>[2022.06]</strong> I am invited to host the VALSE Student Webinar & Panel namely "When CV meets NLP".
                 [<a href="https://www.bilibili.com/video/BV1zF411V7cE?share_source=copy_web">Media</a>] </p>
            <p style="font-size:13px"> <strong>[2022.01]</strong> I am invited to host the VALSE Student Webinar about conference and journal rebuttal.
                 [<a href="https://mp.weixin.qq.com/s/PvtzVhbpcP03mkcfFxGLGw">Media</a>] </p>
            <p style="font-size:13px"> <strong>[2021.07]</strong> I am invited to talk about Network Quantization in Multiple Scenarios at 
                J Ventures (将门创投). [<a href="https://www.techbeat.net/talk-info?id=565">Video</a>] </p>
            <p style="font-size:13px"> <strong>[2021.06]</strong> I am invited to talk about Data-free Quantization at 
                Zhidx (智东西公开课). [<a href="https://course.zhidx.com/c/NmUxMjIxNzhkZGZhOGUyMzY4YzM=">Video</a>] </p>
            <p style="font-size:13px"> <strong>[2021.05]</strong> I am invited to present our <a href="https://arxiv.org/abs/2013.01049">DSG</a> (CVPR 2021 oral) at 
                MSRA Tech Talk. [<a href="https://htqin.github.io/Slides/DSG-MSRA-20201512.pdf">Slides</a>] </p>
            <p style="font-size:13px"> <strong>[2020.04]</strong> I am invited to present our <a href="https://arxiv.org/abs/1909.10788">IR-Net</a> (CVPR 2020) and <a href="https://arxiv.org/abs/2004.03333">survey paper</a> at 
                JD AI Research. [<a href="https://htqin.github.io/Slides/talk-JD-20200414.pdf">Slides</a>] </p>
            <p style="font-size:13px"> <strong>[Fall 2020]</strong> I am the teaching Assistant in Machine Learning (Beihang University).</p>
           </td>
           </tr></tbody>
   </table>

    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td>
          <heading>About Me</heading>
           <p align="justify">In my free time, I like playing Chinese folk music, especially string music (Erhu, Zhonghu, etc.).
               In fact, I am almost a professional Erhu performer.
               I have studied Erhu supervised by Prof. <a href="https://www.ccom.edu.cn/jxyx/myx/myxls/201510/t20151026_35707.html">Zaili Tian</a>, Prof. <a href="https://baike.baidu.com/item/%E9%AB%98%E6%89%AC/5994407">Yang Gao</a>, and  Prof. Qingfu Zhu. 
               I was the vice-president of the Beihang Folk Music Orchestra, here are some of the performance videos of our orchestra [<a href="https://v.qq.com/x/search/?q=%E5%8C%97%E8%88%AA%E6%B0%91%E4%B9%90%E5%9B%A2&stag=txt.playpage.vppdesc">Tencent Video</a>][<a href="https://www.bilibili.com/video/BV1W44y1v7DB?share_source=copy_web">Bilibili</a>].
		   </p>
		   </td></tr>
       </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 25% ">
        <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=O7AjyGXmrOPjngfk_VI9NhWFu7JWskLJlRerwKQdjh4"></script>
    </p></td>
    </tr>
    </tbody>
    </table>
